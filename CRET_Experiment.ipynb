{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2KhdY8LcmIH"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0O6bVUUcwxR"
      },
      "source": [
        "## Required Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h927EScc1tA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torchvision.models as models\n",
        "from typing import List, Tuple\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # for progress bars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z__uc013U3N5"
      },
      "source": [
        "## Video Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqNQQEH4U4jl"
      },
      "outputs": [],
      "source": [
        "class VideoEncoder(nn.Module):\n",
        "    def __init__(self, d_model=768):\n",
        "        super().__init__()\n",
        "        self.spatial_encoder = models.resnet50(pretrained=True)\n",
        "        self.spatial_encoder = nn.Sequential(*list(self.spatial_encoder.children())[:-1])\n",
        "        self.projection = nn.Linear(2048, d_model)\n",
        "\n",
        "        # Modified transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=8,\n",
        "            dim_feedforward=2048,\n",
        "            batch_first=True  # Important for dimension handling\n",
        "        )\n",
        "        self.temporal_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        B, F, C, H, W = x.shape\n",
        "\n",
        "        # Process each frame\n",
        "        x = x.view(B * F, C, H, W)\n",
        "        spatial_features = self.spatial_encoder(x)\n",
        "        spatial_features = spatial_features.squeeze(-1).squeeze(-1)  # Remove spatial dimensions\n",
        "        spatial_features = self.projection(spatial_features)\n",
        "\n",
        "        # Reshape for temporal processing\n",
        "        spatial_features = spatial_features.view(B, F, -1)  # [batch, frames, features]\n",
        "\n",
        "        # Temporal encoding (now with batch_first=True)\n",
        "        temporal_features = self.temporal_encoder(spatial_features)\n",
        "\n",
        "        return temporal_features, spatial_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dylz_SSLVWYA"
      },
      "source": [
        "## CCM Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGuZ1OCnVYAV"
      },
      "outputs": [],
      "source": [
        "class CCMModule(nn.Module):\n",
        "    def __init__(self, d_model=768, num_heads=8, num_queries=8):\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.query_centers = nn.Parameter(torch.randn(num_queries, d_model))\n",
        "\n",
        "        # Modified decoder layer\n",
        "        self.decoder = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=2048,\n",
        "            batch_first=True  # Important for dimension handling\n",
        "        )\n",
        "\n",
        "    def forward(self, video_features: torch.Tensor, text_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        batch_size = video_features.size(0)\n",
        "\n",
        "        # Expand queries for batch dimension\n",
        "        query = self.query_centers.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        # Apply decoder to both modalities\n",
        "        video_aligned = self.decoder(query, video_features)\n",
        "        text_aligned = self.decoder(query, text_features)\n",
        "\n",
        "        return video_aligned, text_aligned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBjhimDdVZdv"
      },
      "source": [
        "## CRET Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjaCw6nNVcvF"
      },
      "outputs": [],
      "source": [
        "class CRET(nn.Module):\n",
        "    def __init__(self, d_model=768):\n",
        "        super().__init__()\n",
        "        self.video_encoder = VideoEncoder(d_model)\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.ccm = CCMModule(d_model)\n",
        "\n",
        "    def forward(self, video: torch.Tensor, text_ids: torch.Tensor, text_mask: torch.Tensor):\n",
        "        temporal_features, spatial_features = self.video_encoder(video)\n",
        "        text_outputs = self.text_encoder(text_ids, attention_mask=text_mask)\n",
        "        text_features = text_outputs.last_hidden_state\n",
        "        video_aligned, text_aligned = self.ccm(spatial_features, text_features)\n",
        "        video_global = temporal_features.mean(1)\n",
        "        text_global = text_outputs.pooler_output\n",
        "        return {\n",
        "            'video_aligned': video_aligned,\n",
        "            'text_aligned': text_aligned,\n",
        "            'video_global': video_global,\n",
        "            'text_global': text_global\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ytAhfAoVgWv"
      },
      "source": [
        "## GEES Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xto08ysHVi_a"
      },
      "outputs": [],
      "source": [
        "class GEESLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, video_features: torch.Tensor, text_features: torch.Tensor):\n",
        "        # Make sure video_features has the right shape [batch_size, seq_len, features]\n",
        "        if len(video_features.shape) == 2:\n",
        "            video_features = video_features.unsqueeze(1)\n",
        "\n",
        "        # Compute mean over sequence dimension (if exists)\n",
        "        if len(video_features.shape) == 3:\n",
        "            video_mean = video_features.mean(1)\n",
        "        else:\n",
        "            video_mean = video_features\n",
        "\n",
        "        # Compute covariance\n",
        "        batch_size = video_features.size(0)\n",
        "        feature_dim = video_features.size(-1)\n",
        "\n",
        "        # Reshape video features for batch matrix multiplication\n",
        "        video_features_reshaped = video_features.view(batch_size, -1, feature_dim)\n",
        "\n",
        "        # Compute covariance for each item in the batch\n",
        "        video_cov = torch.bmm(video_features_reshaped.transpose(1, 2),\n",
        "                            video_features_reshaped) / video_features_reshaped.size(1)\n",
        "\n",
        "        # Compute similarity scores\n",
        "        sim = torch.mm(text_features, video_mean.t())\n",
        "\n",
        "        # Compute covariance term\n",
        "        cov_term = 0.5 * torch.bmm(\n",
        "            torch.bmm(text_features.unsqueeze(1), video_cov),\n",
        "            text_features.unsqueeze(-1)\n",
        "        ).squeeze()\n",
        "\n",
        "        # Compute logits and loss\n",
        "        logits = sim + cov_term\n",
        "        labels = torch.arange(batch_size).to(video_features.device)\n",
        "\n",
        "        return F.cross_entropy(logits, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgW8wCKJWHLT"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_RNeZWIWJ-8"
      },
      "outputs": [],
      "source": [
        "class MSRVTTDataset(Dataset):\n",
        "    def __init__(self, video_paths, captions, tokenizer):\n",
        "        self.video_paths = video_paths\n",
        "        self.captions = captions\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Add cv2 import inside the class\n",
        "        import cv2\n",
        "        self.cv2 = cv2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = self.load_video(self.video_paths[idx])\n",
        "        caption = self.captions[idx]\n",
        "        tokens = self.tokenizer(\n",
        "            caption,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return video, tokens.input_ids.squeeze(0), tokens.attention_mask.squeeze(0)\n",
        "\n",
        "    def load_video(self, path):\n",
        "        import numpy as np\n",
        "        frames = []\n",
        "\n",
        "        # Load video with OpenCV\n",
        "        cap = self.cv2.VideoCapture(path)\n",
        "\n",
        "        # Sample 4 frames as mentioned in the paper\n",
        "        total_frames = int(cap.get(self.cv2.CAP_PROP_FRAME_COUNT))\n",
        "        indices = np.linspace(0, total_frames-1, 4, dtype=int)\n",
        "\n",
        "        for frame_idx in indices:\n",
        "            cap.set(self.cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                # Convert BGR to RGB\n",
        "                frame = self.cv2.cvtColor(frame, self.cv2.COLOR_BGR2RGB)\n",
        "                # Resize to 224x224\n",
        "                frame = self.cv2.resize(frame, (224, 224))\n",
        "                # Convert to tensor and normalize\n",
        "                frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
        "                frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Stack frames\n",
        "        if len(frames) == 0:  # Handle empty video case\n",
        "            return torch.zeros((4, 3, 224, 224))\n",
        "\n",
        "        video_tensor = torch.stack(frames)\n",
        "        return video_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D10oN3Tsxj0p"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9VaM55xnCi"
      },
      "outputs": [],
      "source": [
        "def train_cret(model, train_loader, num_epochs=2, device='cpu'):\n",
        "    print(f\"Training on device: {device}\")\n",
        "    model = model.to(device)\n",
        "    gees_loss = GEESLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (videos, text_ids, text_mask) in enumerate(train_loader):\n",
        "            try:\n",
        "                # Move to device\n",
        "                videos = videos.to(device).float()\n",
        "                text_ids = text_ids.to(device)\n",
        "                text_mask = text_mask.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(videos, text_ids, text_mask)\n",
        "\n",
        "                # Print shapes for debugging\n",
        "                if batch_idx == 0:\n",
        "                    print(f\"Video global shape: {outputs['video_global'].shape}\")\n",
        "                    print(f\"Text global shape: {outputs['text_global'].shape}\")\n",
        "                    print(f\"Video aligned shape: {outputs['video_aligned'].shape}\")\n",
        "                    print(f\"Text aligned shape: {outputs['text_aligned'].shape}\")\n",
        "\n",
        "                # Compute losses\n",
        "                ccm_loss = F.cosine_embedding_loss(\n",
        "                    outputs['video_aligned'].view(-1, 768),\n",
        "                    outputs['text_aligned'].view(-1, 768),\n",
        "                    torch.ones(outputs['video_aligned'].size(0) * outputs['video_aligned'].size(1)).to(device)\n",
        "                )\n",
        "\n",
        "                gees_loss_val = gees_loss(outputs['video_global'], outputs['text_global'])\n",
        "                loss = ccm_loss + gees_loss_val\n",
        "\n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                print(f\"Shapes:\")\n",
        "                for k, v in outputs.items():\n",
        "                    print(f\"{k}: {v.shape}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch} completed, Average Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDfiItClx_r5"
      },
      "source": [
        "## Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0OH_udvyEIm"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Load MSRVTT dataset\n",
        "    msrvtt_root = './data/MSRVTT'\n",
        "    # Create the videos directory if it doesn't exist\n",
        "    os.makedirs(os.path.join(msrvtt_root, 'videos'), exist_ok=True)\n",
        "\n",
        "    # Download video data and captions if they don't exist\n",
        "    if not os.listdir(os.path.join(msrvtt_root, 'videos')):\n",
        "        print(\"Downloading video data...\")\n",
        "        # Replace <updated_video_data_download_link> with the actual download link\n",
        "        os.system('wget -P ./data/MSRVTT/videos <updated_video_data_download_link>')\n",
        "    if not os.path.exists(os.path.join(msrvtt_root, 'captions.json')):\n",
        "        print(\"Downloading captions...\")\n",
        "        os.system('wget -P ./data/MSRVTT https://www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/msrvtt_data.json')\n",
        "\n",
        "    video_paths = [os.path.join(msrvtt_root, 'videos', file) for file in os.listdir(os.path.join(msrvtt_root, 'videos')) if file.endswith('.mp4')]\n",
        "\n",
        "    with open(os.path.join(msrvtt_root, 'captions.json'), 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "\n",
        "    captions = [captions_data[video_id[:-4]] for video_id in os.listdir(os.path.join(msrvtt_root, 'videos')) if video_id.endswith('.mp4')]\n",
        "    captions = [[caption['caption'] for caption in captions_per_video] for captions_per_video in captions]\n",
        "    # Create dataset and dataloader\n",
        "    dataset = MSRVTTDataset(video_paths, captions, tokenizer)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = CRET()\n",
        "\n",
        "    # Train model\n",
        "    train_cret(model, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j4-BKzM9ADH"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Set up paths for your Colab environment\n",
        "    msrvtt_root = '/content/MSRVTT'  # Adjust this to your actual path\n",
        "\n",
        "    # Get video paths and captions\n",
        "    video_paths = [os.path.join(msrvtt_root, 'videos', file)\n",
        "                  for file in os.listdir(os.path.join(msrvtt_root, 'videos'))\n",
        "                  if file.endswith('.mp4')]\n",
        "\n",
        "    # Load captions\n",
        "    with open(os.path.join(msrvtt_root, 'annotation', 'captions.json'), 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = MSRVTTDataset(video_paths[:100], captions[:100], tokenizer)  # Start with 100 videos for testing\n",
        "    train_loader = DataLoader(dataset, batch_size=8, shuffle=True)  # Smaller batch size for Colab\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = CRET().to(device)\n",
        "\n",
        "    # Train model\n",
        "    train_cret(model, train_loader, num_epochs=5, device=device)  # Start with 5 epochs for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxRjTtBt9A5R",
        "outputId": "f18ecd48-be1e-40b3-e9ff-93142f0487d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K74G13_BFB8S",
        "outputId": "8dd9f59d-1077-4cd7-bb9e-ac3a9ede0f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: /content\n",
            "\n",
            "Contents of current directory:\n",
            "MSRVTT\tsample_data\n",
            "\n",
            "Contents of /content directory:\n",
            "MSRVTT\tsample_data\n"
          ]
        }
      ],
      "source": [
        "# Check current directory and list files\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"\\nContents of current directory:\")\n",
        "!ls\n",
        "\n",
        "print(\"\\nContents of /content directory:\")\n",
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzTwqzHzFI1E",
        "outputId": "4601fd1d-6051-4dfb-f9f1-1eb1f05a1dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of MSRVTT folder:\n",
            "video0.mp4    video116.mp4  video1.mp4\t video36.mp4  video52.mp4  video69.mp4\tvideo85.mp4\n",
            "video100.mp4  video117.mp4  video20.mp4  video37.mp4  video53.mp4  video6.mp4\tvideo86.mp4\n",
            "video101.mp4  video118.mp4  video21.mp4  video38.mp4  video54.mp4  video70.mp4\tvideo87.mp4\n",
            "video102.mp4  video119.mp4  video22.mp4  video39.mp4  video55.mp4  video71.mp4\tvideo88.mp4\n",
            "video103.mp4  video11.mp4   video23.mp4  video3.mp4   video56.mp4  video72.mp4\tvideo89.mp4\n",
            "video104.mp4  video120.mp4  video24.mp4  video40.mp4  video57.mp4  video73.mp4\tvideo8.mp4\n",
            "video105.mp4  video121.mp4  video25.mp4  video41.mp4  video58.mp4  video74.mp4\tvideo90.mp4\n",
            "video106.mp4  video122.mp4  video26.mp4  video42.mp4  video59.mp4  video75.mp4\tvideo91.mp4\n",
            "video107.mp4  video123.mp4  video27.mp4  video43.mp4  video5.mp4   video76.mp4\tvideo92.mp4\n",
            "video108.mp4  video124.mp4  video28.mp4  video44.mp4  video60.mp4  video77.mp4\tvideo93.mp4\n",
            "video109.mp4  video12.mp4   video29.mp4  video45.mp4  video61.mp4  video78.mp4\tvideo94.mp4\n",
            "video10.mp4   video13.mp4   video2.mp4\t video46.mp4  video62.mp4  video79.mp4\tvideo95.mp4\n",
            "video110.mp4  video14.mp4   video30.mp4  video47.mp4  video63.mp4  video7.mp4\tvideo96.mp4\n",
            "video111.mp4  video15.mp4   video31.mp4  video48.mp4  video64.mp4  video80.mp4\tvideo97.mp4\n",
            "video112.mp4  video16.mp4   video32.mp4  video49.mp4  video65.mp4  video81.mp4\tvideo98.mp4\n",
            "video113.mp4  video17.mp4   video33.mp4  video4.mp4   video66.mp4  video82.mp4\tvideo99.mp4\n",
            "video114.mp4  video18.mp4   video34.mp4  video50.mp4  video67.mp4  video83.mp4\tvideo9.mp4\n",
            "video115.mp4  video19.mp4   video35.mp4  video51.mp4  video68.mp4  video84.mp4\n"
          ]
        }
      ],
      "source": [
        "print(\"Contents of MSRVTT folder:\")\n",
        "!ls /content/MSRVTT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790,
          "referenced_widgets": [
            "9137c6d0134e4a4db7b8c9b63fcbd2ec",
            "de0a747d43bb46d9a35258e83d29ef53",
            "078df586003a46e984efd249ebeca620",
            "8d895abbaee04237b6c6bf684e9f60db",
            "4ed5db89b2664304bd9bd77796e8c393",
            "ada9f719fd2e4130816dc65625c5d37c",
            "3e0e1e90dfee42b597a2f6db21ef62a8",
            "de0ee00cfbf04a0fbf1940afbc593fbd",
            "6e653316f2ae40348653a7a1bb564d3c",
            "36fc0dffeb7c4974a1d4c48fd7da763a",
            "880f12566a634dd29ada4e5cf7f30a8c",
            "8c194d4304cb453eaf4f040adca5760a",
            "d148c6f3c096434da0c72cfb4a2f02a8",
            "dd411e6422cb48d794f06d505e0a68d1",
            "71557515b017488182ccac996f3e3c43",
            "8381bf6400f54d428aa9871b9379f2ab",
            "df6c0a292fa44ddc82c3ece0aace2e92",
            "2175d2bc434e4edcb4990513a63e53ca",
            "9f7b35229bb44583ba3b683e98eb13c6",
            "f9d91c05721647ab9aa60cd569509f02",
            "deee248424164371a0019036677d69db",
            "d8284d01b1804ba1a4a1733e7f23a7fe",
            "5f9744a72db94c54b3cb146d0ba5b259",
            "c868fb2756334fdfa509d53367dd0314",
            "7760cc4bc097434395671a68be9e322c",
            "262cfdc0e3aa46ae8a1fda3514d89ede",
            "9877a0c76354462bbe1f88107b354fc3",
            "4778cb76d4d740c0a274da5c71829986",
            "bd0dc8efd78c42a2b11516325eb3294c",
            "f33236c245fa4e0488724681965de166",
            "985381c3cc214d299ec2a390c4dd5857",
            "6fda19017ec14e0d8faa29ff6a6eaf15",
            "eabc566ddc194ba782508d4ed9f5cadd",
            "b3ee5338e874420eba594476c274159c",
            "f16450b38ec449059eef3942da4a498c",
            "97d1577a08ef4fa7aac31dc7d1270eb1",
            "091fe204837441d2aed7fe2fbc7cc6e8",
            "fc56f935fa6c4fe9b70a50f22bd24694",
            "b0d60b63b05e488baffff237f140abd4",
            "91f3051b7dc44da78da8ab516f7bbff9",
            "2d7d09eff1db4d4cbab3a233b20f019e",
            "beb42a934644434c9c62633b469b8bd0",
            "5b178540e0d54b74927a06fd930be320",
            "47941bbb99b341e5b2189376ee914fba"
          ]
        },
        "id": "dp-Xcqv69C33",
        "outputId": "fe9efed9-b507-4cdd-cd57-61d0e5776ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing dataset loading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9137c6d0134e4a4db7b8c9b63fcbd2ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c194d4304cb453eaf4f040adca5760a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f9744a72db94c54b3cb146d0ba5b259",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3ee5338e874420eba594476c274159c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checking contents of MSRVTT directory:\n",
            "video0.mp4    video116.mp4  video1.mp4\t video36.mp4  video52.mp4  video69.mp4\tvideo85.mp4\n",
            "video100.mp4  video117.mp4  video20.mp4  video37.mp4  video53.mp4  video6.mp4\tvideo86.mp4\n",
            "video101.mp4  video118.mp4  video21.mp4  video38.mp4  video54.mp4  video70.mp4\tvideo87.mp4\n",
            "video102.mp4  video119.mp4  video22.mp4  video39.mp4  video55.mp4  video71.mp4\tvideo88.mp4\n",
            "video103.mp4  video11.mp4   video23.mp4  video3.mp4   video56.mp4  video72.mp4\tvideo89.mp4\n",
            "video104.mp4  video120.mp4  video24.mp4  video40.mp4  video57.mp4  video73.mp4\tvideo8.mp4\n",
            "video105.mp4  video121.mp4  video25.mp4  video41.mp4  video58.mp4  video74.mp4\tvideo90.mp4\n",
            "video106.mp4  video122.mp4  video26.mp4  video42.mp4  video59.mp4  video75.mp4\tvideo91.mp4\n",
            "video107.mp4  video123.mp4  video27.mp4  video43.mp4  video5.mp4   video76.mp4\tvideo92.mp4\n",
            "video108.mp4  video124.mp4  video28.mp4  video44.mp4  video60.mp4  video77.mp4\tvideo93.mp4\n",
            "video109.mp4  video12.mp4   video29.mp4  video45.mp4  video61.mp4  video78.mp4\tvideo94.mp4\n",
            "video10.mp4   video13.mp4   video2.mp4\t video46.mp4  video62.mp4  video79.mp4\tvideo95.mp4\n",
            "video110.mp4  video14.mp4   video30.mp4  video47.mp4  video63.mp4  video7.mp4\tvideo96.mp4\n",
            "video111.mp4  video15.mp4   video31.mp4  video48.mp4  video64.mp4  video80.mp4\tvideo97.mp4\n",
            "video112.mp4  video16.mp4   video32.mp4  video49.mp4  video65.mp4  video81.mp4\tvideo98.mp4\n",
            "video113.mp4  video17.mp4   video33.mp4  video4.mp4   video66.mp4  video82.mp4\tvideo99.mp4\n",
            "video114.mp4  video18.mp4   video34.mp4  video50.mp4  video67.mp4  video83.mp4\tvideo9.mp4\n",
            "video115.mp4  video19.mp4   video35.mp4  video51.mp4  video68.mp4  video84.mp4\n",
            "\n",
            "Found 125 video files\n",
            "\n",
            "Trying to load a single video...\n",
            "\n",
            "Successfully loaded:\n",
            "Video tensor shape: torch.Size([4, 3, 224, 224])\n",
            "Text IDs shape: torch.Size([128])\n",
            "Text mask shape: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "def test_dataset():\n",
        "    print(\"Testing dataset loading...\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Set paths\n",
        "    msrvtt_root = '/content/MSRVTT'\n",
        "    print(f\"\\nChecking contents of MSRVTT directory:\")\n",
        "    !ls {msrvtt_root}\n",
        "\n",
        "    # Get video paths\n",
        "    video_paths = [os.path.join(msrvtt_root, f) for f in os.listdir(msrvtt_root) if f.endswith('.mp4')]\n",
        "    print(f\"\\nFound {len(video_paths)} video files\")\n",
        "\n",
        "    # For testing, let's use just one video with a dummy caption\n",
        "    test_dataset = MSRVTTDataset([video_paths[0]], ['a person is doing something'], tokenizer)\n",
        "    print(\"\\nTrying to load a single video...\")\n",
        "\n",
        "    try:\n",
        "        video, text_ids, text_mask = test_dataset[0]\n",
        "        print(f\"\\nSuccessfully loaded:\")\n",
        "        print(f\"Video tensor shape: {video.shape}\")\n",
        "        print(f\"Text IDs shape: {text_ids.shape}\")\n",
        "        print(f\"Text mask shape: {text_mask.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading video: {e}\")\n",
        "\n",
        "test_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g83S-QdaF9DZ"
      },
      "outputs": [],
      "source": [
        "def create_test_annotations():\n",
        "    video_files = [f for f in os.listdir('/content/MSRVTT') if f.endswith('.mp4')]\n",
        "    annotations = {}\n",
        "\n",
        "    for video_file in video_files:\n",
        "        video_id = video_file[:-4]  # Remove .mp4\n",
        "        annotations[video_id] = [{\n",
        "            'caption': f'This is a test caption for video {video_id}',\n",
        "            'video_id': video_id\n",
        "        }]\n",
        "\n",
        "    with open('/content/MSRVTT/test_annotations.json', 'w') as f:\n",
        "        json.dump(annotations, f)\n",
        "\n",
        "    print(\"Created test annotations file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQmbrvcDGHDA",
        "outputId": "2dc1e653-9095-4e02-a3eb-f3173b1d54a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created test annotations file\n"
          ]
        }
      ],
      "source": [
        "# Create test annotations\n",
        "def create_test_annotations():\n",
        "    video_files = [f for f in os.listdir('/content/MSRVTT') if f.endswith('.mp4')]\n",
        "    annotations = {}\n",
        "\n",
        "    for video_file in video_files:\n",
        "        video_id = video_file[:-4]  # Remove .mp4\n",
        "        annotations[video_id] = [{\n",
        "            'caption': f'This is a test caption for video {video_id}',\n",
        "            'video_id': video_id\n",
        "        }]\n",
        "\n",
        "    with open('/content/MSRVTT/test_annotations.json', 'w') as f:\n",
        "        json.dump(annotations, f)\n",
        "\n",
        "    print(\"Created test annotations file\")\n",
        "\n",
        "create_test_annotations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRAUaiJEGIlT"
      },
      "outputs": [],
      "source": [
        "class MSRVTTDataset(Dataset):\n",
        "    def __init__(self, video_paths, captions, tokenizer):\n",
        "        self.video_paths = video_paths\n",
        "        self.captions = captions\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Add cv2 import inside the class\n",
        "        import cv2\n",
        "        self.cv2 = cv2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = self.load_video(self.video_paths[idx])\n",
        "        caption = self.captions[idx]\n",
        "        tokens = self.tokenizer(\n",
        "            caption,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return video, tokens.input_ids.squeeze(0), tokens.attention_mask.squeeze(0)\n",
        "\n",
        "    def load_video(self, path):\n",
        "        import numpy as np\n",
        "        frames = []\n",
        "\n",
        "        try:\n",
        "            # Load video with OpenCV\n",
        "            cap = self.cv2.VideoCapture(path)\n",
        "\n",
        "            # Get total frames\n",
        "            total_frames = int(cap.get(self.cv2.CAP_PROP_FRAME_COUNT))\n",
        "            if total_frames == 0:\n",
        "                # Return dummy frames if video is empty\n",
        "                return torch.zeros((4, 3, 224, 224))\n",
        "\n",
        "            # Sample 4 evenly spaced frames\n",
        "            indices = np.linspace(0, total_frames-1, 4, dtype=int)\n",
        "\n",
        "            for frame_idx in indices:\n",
        "                cap.set(self.cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "                ret, frame = cap.read()\n",
        "                if ret:\n",
        "                    # Convert BGR to RGB\n",
        "                    frame = self.cv2.cvtColor(frame, self.cv2.COLOR_BGR2RGB)\n",
        "                    # Resize to 224x224\n",
        "                    frame = self.cv2.resize(frame, (224, 224))\n",
        "                    # Convert to tensor and normalize\n",
        "                    frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
        "                    frames.append(frame)\n",
        "                else:\n",
        "                    # If frame read fails, append zero frame\n",
        "                    frames.append(torch.zeros((3, 224, 224)))\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            # Ensure we have exactly 4 frames\n",
        "            while len(frames) < 4:\n",
        "                frames.append(torch.zeros((3, 224, 224)))\n",
        "            frames = frames[:4]  # Take only first 4 if we somehow got more\n",
        "\n",
        "            # Stack frames\n",
        "            video_tensor = torch.stack(frames)\n",
        "            return video_tensor\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading video {path}: {str(e)}\")\n",
        "            # Return dummy frames if loading fails\n",
        "            return torch.zeros((4, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31kii1tqJi_y"
      },
      "source": [
        "## Monitor Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA_SCFQjJkiD"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm  # For progress bars\n",
        "\n",
        "def train_cret(model, train_loader, num_epochs=2, device='cpu'):\n",
        "    print(f\"Training on device: {device}\")\n",
        "    model = model.to(device)\n",
        "    gees_loss = GEESLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        for batch_idx, (videos, text_ids, text_mask) in enumerate(progress_bar):\n",
        "            try:\n",
        "                videos = videos.to(device).float()\n",
        "                text_ids = text_ids.to(device)\n",
        "                text_mask = text_mask.to(device)\n",
        "\n",
        "                outputs = model(videos, text_ids, text_mask)\n",
        "\n",
        "                ccm_loss = F.cosine_embedding_loss(\n",
        "                    outputs['video_aligned'].view(-1, 768),\n",
        "                    outputs['text_aligned'].view(-1, 768),\n",
        "                    torch.ones(outputs['video_aligned'].size(0) * outputs['video_aligned'].size(1)).to(device)\n",
        "                )\n",
        "\n",
        "                gees_loss_val = gees_loss(outputs['video_global'], outputs['text_global'])\n",
        "                loss = ccm_loss + gees_loss_val\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1} completed, Average Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY39qLjLJmSt"
      },
      "source": [
        "## Preserve Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVe9l_gmJqdp"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, filename)\n",
        "    print(f\"Checkpoint saved: {filename}\")\n",
        "\n",
        "def train_cret(model, train_loader, num_epochs=2, device='cpu'):\n",
        "    print(f\"Training on device: {device}\")\n",
        "    model = model.to(device)\n",
        "    gees_loss = GEESLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        for batch_idx, (videos, text_ids, text_mask) in enumerate(progress_bar):\n",
        "            try:\n",
        "                videos = videos.to(device).float()\n",
        "                text_ids = text_ids.to(device)\n",
        "                text_mask = text_mask.to(device)\n",
        "\n",
        "                outputs = model(videos, text_ids, text_mask)\n",
        "\n",
        "                ccm_loss = F.cosine_embedding_loss(\n",
        "                    outputs['video_aligned'].view(-1, 768),\n",
        "                    outputs['text_aligned'].view(-1, 768),\n",
        "                    torch.ones(outputs['video_aligned'].size(0) * outputs['video_aligned'].size(1)).to(device)\n",
        "                )\n",
        "\n",
        "                gees_loss_val = gees_loss(outputs['video_global'], outputs['text_global'])\n",
        "                loss = ccm_loss + gees_loss_val\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1} completed, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        save_checkpoint(model, optimizer, epoch, avg_loss,\n",
        "                      f'cret_checkpoint_epoch_{epoch+1}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h44ldkPSJwr5"
      },
      "source": [
        "## Test Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC3y3o4sJyGi"
      },
      "outputs": [],
      "source": [
        "def test_retrieval(model, video_path, query_text, tokenizer, device='cpu'):\n",
        "    model.eval()\n",
        "\n",
        "    # Load and process video\n",
        "    dataset = MSRVTTDataset([video_path], [query_text], tokenizer)\n",
        "    video, text_ids, text_mask = dataset[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        video = video.unsqueeze(0).to(device)\n",
        "        text_ids = text_ids.unsqueeze(0).to(device)\n",
        "        text_mask = text_mask.unsqueeze(0).to(device)\n",
        "\n",
        "        outputs = model(video, text_ids, text_mask)\n",
        "\n",
        "        # Compute similarity score\n",
        "        video_emb = outputs['video_global']\n",
        "        text_emb = outputs['text_global']\n",
        "        similarity = F.cosine_similarity(video_emb, text_emb)\n",
        "\n",
        "        return similarity.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wDi8JPgJ-t2"
      },
      "source": [
        "## Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "f0da983287c3494fac27f8815cc5b8cb",
            "7884f759f8b849a39079b9048e2d8c34",
            "a02604c388f24c1fbfc04b4277ca3ce1",
            "ea1e5e2d27f34ce6b59b9aceed530a99",
            "366873965c5741b285403ad21fc76c87",
            "6d1dd8c890ca4f8fb644908def62dfd9",
            "a6c594d3820c43e98341b1842364c434",
            "6bbed8b1874a48bfa161a0fa59e5075d",
            "f8ac2448be6f4f598f74018c5188fec2",
            "d03a96d433f34598949e8e253a3332f0",
            "0986586914834c6794ee308eacd775e0"
          ]
        },
        "id": "4H6VEo83J_6Q",
        "outputId": "a8cf64b4-6326-4103-a2be-f90aaaf748bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 125 video files\n",
            "Created test annotations file\n",
            "Prepared 125 video-caption pairs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 62.3MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0da983287c3494fac27f8815cc5b8cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2: 100%|██████████| 63/63 [10:02<00:00,  9.57s/it, loss=0.00134]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed, Average Loss: 168.9317\n",
            "Checkpoint saved: cret_checkpoint_epoch_1.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 63/63 [10:07<00:00,  9.64s/it, loss=0.000745]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed, Average Loss: 12.6540\n",
            "Checkpoint saved: cret_checkpoint_epoch_2.pt\n"
          ]
        }
      ],
      "source": [
        "def create_test_annotations():\n",
        "    # Get list of videos\n",
        "    video_files = sorted([f for f in os.listdir('MSRVTT') if f.endswith('.mp4')])\n",
        "    print(f\"Found {len(video_files)} video files\")\n",
        "\n",
        "    # Create annotations dictionary\n",
        "    annotations = {}\n",
        "    for video_file in video_files:\n",
        "        video_id = video_file[:-4]  # Remove .mp4\n",
        "        # Store multiple captions per video for better training\n",
        "        annotations[video_id] = [\n",
        "            {'caption': f'A video showing content from {video_id}'},\n",
        "            {'caption': f'This is video file {video_id}'},\n",
        "            {'caption': f'A clip from video sequence {video_id}'}\n",
        "        ]\n",
        "\n",
        "    # Save annotations\n",
        "    with open('MSRVTT/test_annotations.json', 'w') as f:\n",
        "        json.dump(annotations, f)\n",
        "\n",
        "    print(\"Created test annotations file\")\n",
        "    return annotations  # Added this line to return the annotations\n",
        "\n",
        "def main():\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Set correct path\n",
        "    msrvtt_root = 'MSRVTT'\n",
        "    video_paths = sorted([os.path.join(msrvtt_root, f) for f in os.listdir(msrvtt_root) if f.endswith('.mp4')])\n",
        "    annotations = create_test_annotations()  # Now this will receive the returned annotations\n",
        "\n",
        "    # Get captions for each video\n",
        "    captions = []\n",
        "    for video_path in video_paths:\n",
        "        video_id = os.path.basename(video_path)[:-4]\n",
        "        if video_id in annotations:\n",
        "            caption = annotations[video_id][0]['caption']\n",
        "            captions.append(caption)\n",
        "\n",
        "    print(f\"Prepared {len(captions)} video-caption pairs\")\n",
        "    dataset = MSRVTTDataset(video_paths[:len(captions)], captions, tokenizer)\n",
        "    train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = CRET().to(device)\n",
        "\n",
        "    # Train model with progress bar\n",
        "    train_cret(model, train_loader, num_epochs=2, device=device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCXMwcww60c-"
      },
      "source": [
        "## CRET Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYaP7RcS9usW",
        "outputId": "1c8b7cc3-41ec-45f7-cf94-1237d89fec4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current variables:\n",
            "BertModel\t BertTokenizer\t CCMModule\t CRET\t DataLoader\t Dataset\t F\t GEESLoss\t List\t \n",
            "MSRVTTDataset\t Tuple\t VideoEncoder\t create_test_annotations\t cv2\t json\t main\t models\t nn\t \n",
            "np\t os\t save_checkpoint\t test_dataset\t test_retrieval\t torch\t tqdm\t train_cret\t \n"
          ]
        }
      ],
      "source": [
        "# Check what variables exist\n",
        "print(\"Current variables:\")\n",
        "%who"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKtnWZGTF-nz",
        "outputId": "ac65adc2-df19-486a-b3f1-e16949fd4442"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torchvision.models as models\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Define VideoEncoder\n",
        "class VideoEncoder(nn.Module):\n",
        "    def __init__(self, d_model=768):\n",
        "        super().__init__()\n",
        "        self.spatial_encoder = models.resnet50(pretrained=True)\n",
        "        self.spatial_encoder = nn.Sequential(*list(self.spatial_encoder.children())[:-1])\n",
        "        self.projection = nn.Linear(2048, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8)\n",
        "        self.temporal_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        B, F, C, H, W = x.shape\n",
        "        x = x.view(B*F, C, H, W)\n",
        "        spatial_features = self.spatial_encoder(x)\n",
        "        spatial_features = spatial_features.view(B, F, -1)\n",
        "        spatial_features = self.projection(spatial_features)\n",
        "        temporal_features = self.temporal_encoder(spatial_features.transpose(0, 1)).transpose(0, 1)\n",
        "        return temporal_features, spatial_features\n",
        "\n",
        "# 2. Define CCMModule\n",
        "class CCMModule(nn.Module):\n",
        "    def __init__(self, d_model=768, num_heads=8, num_queries=8):\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.query_centers = nn.Parameter(torch.randn(num_queries, d_model))\n",
        "        self.decoder = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads)\n",
        "\n",
        "    def forward(self, video_features: torch.Tensor, text_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        query = self.query_centers.unsqueeze(0).expand(video_features.size(0), -1, -1)\n",
        "        video_aligned = self.decoder(query, video_features)\n",
        "        text_aligned = self.decoder(query, text_features)\n",
        "        return video_aligned, text_aligned\n",
        "\n",
        "# 3. Define CRET\n",
        "class CRET(nn.Module):\n",
        "    def __init__(self, d_model=768):\n",
        "        super().__init__()\n",
        "        self.video_encoder = VideoEncoder(d_model)\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.ccm = CCMModule(d_model)\n",
        "\n",
        "    def forward(self, video: torch.Tensor, text_ids: torch.Tensor, text_mask: torch.Tensor):\n",
        "        temporal_features, spatial_features = self.video_encoder(video)\n",
        "        text_outputs = self.text_encoder(text_ids, attention_mask=text_mask)\n",
        "        text_features = text_outputs.last_hidden_state\n",
        "        video_aligned, text_aligned = self.ccm(spatial_features, text_features)\n",
        "        video_global = temporal_features.mean(1)\n",
        "        text_global = text_outputs.pooler_output\n",
        "        return {\n",
        "            'video_aligned': video_aligned,\n",
        "            'text_aligned': text_aligned,\n",
        "            'video_global': video_global,\n",
        "            'text_global': text_global\n",
        "        }\n",
        "\n",
        "# 4. Define GEESLoss\n",
        "class GEESLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, video_features: torch.Tensor, text_features: torch.Tensor):\n",
        "        video_mean = video_features.mean(1)\n",
        "        video_cov = torch.bmm(video_features.transpose(1, 2), video_features) / video_features.size(1)\n",
        "        sim = torch.mm(text_features, video_mean.t())\n",
        "        cov_term = 0.5 * torch.bmm(torch.bmm(text_features.unsqueeze(1), video_cov), text_features.unsqueeze(-1)).squeeze()\n",
        "        logits = sim + cov_term\n",
        "        labels = torch.arange(video_features.size(0)).to(video_features.device)\n",
        "        return F.cross_entropy(logits, labels)\n",
        "\n",
        "# 5. Define MSRVTTDataset\n",
        "class MSRVTTDataset(Dataset):\n",
        "    def __init__(self, video_paths, captions, tokenizer):\n",
        "        self.video_paths = video_paths\n",
        "        self.captions = captions\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = self.load_video(self.video_paths[idx])\n",
        "        caption = self.captions[idx]\n",
        "        tokens = self.tokenizer(caption, padding='max_length', truncation=True,\n",
        "                              max_length=128, return_tensors='pt')\n",
        "        return video, tokens.input_ids.squeeze(0), tokens.attention_mask.squeeze(0)\n",
        "\n",
        "    def load_video(self, path):\n",
        "        import cv2\n",
        "        frames = []\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        indices = np.linspace(0, total_frames-1, 4, dtype=int)\n",
        "\n",
        "        for frame_idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, (224, 224))\n",
        "                frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
        "                frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "        video_tensor = torch.stack(frames)\n",
        "        return video_tensor\n",
        "\n",
        "# Now initialize model and continue with your code\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CRET().to(device)\n",
        "\n",
        "# Initialize tokenizer and continue with rest of your code..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNxV4kQzHC94",
        "outputId": "fd586204-19d0-4012-f1ab-fc533e4ff730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to cret_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model_save_path = 'cret_model.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20fg-930HcsN"
      },
      "outputs": [],
      "source": [
        "# 1. First update VideoEncoder\n",
        "class VideoEncoder(nn.Module):\n",
        "    def __init__(self, d_model=768):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # Spatial encoder (ResNet)\n",
        "        self.spatial_encoder = models.resnet50(pretrained=True)\n",
        "        self.spatial_encoder = nn.Sequential(*list(self.spatial_encoder.children())[:-1])\n",
        "        self.projection = nn.Linear(2048, d_model)\n",
        "\n",
        "        # Temporal encoder with fixed parameters\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=8,\n",
        "            batch_first=True,\n",
        "            dim_feedforward=2048\n",
        "        )\n",
        "        self.temporal_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        B, F, C, H, W = x.shape  # Batch, Frames, Channels, Height, Width\n",
        "\n",
        "        # Process each frame\n",
        "        x = x.view(B * F, C, H, W)\n",
        "        x = self.spatial_encoder(x)\n",
        "        x = x.squeeze(-1).squeeze(-1)  # Remove spatial dimensions\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Reshape for temporal processing\n",
        "        x = x.view(B, F, -1)  # [batch, frames, features]\n",
        "\n",
        "        # Apply temporal encoding\n",
        "        temporal_features = self.temporal_encoder(x)\n",
        "\n",
        "        return temporal_features, x\n",
        "\n",
        "# 2. Update CCMModule\n",
        "class CCMModule(nn.Module):\n",
        "    def __init__(self, d_model=768, num_heads=8, num_queries=8):\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Learnable query centers\n",
        "        self.query_centers = nn.Parameter(torch.randn(1, num_queries, d_model))\n",
        "\n",
        "        # Transformer decoder layer with fixed parameters\n",
        "        self.decoder = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            batch_first=True,\n",
        "            dim_feedforward=2048\n",
        "        )\n",
        "\n",
        "    def forward(self, video_features: torch.Tensor, text_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        batch_size = video_features.size(0)\n",
        "\n",
        "        # Expand queries for batch\n",
        "        query = self.query_centers.expand(batch_size, -1, -1)\n",
        "\n",
        "        # Apply decoder to both modalities\n",
        "        video_aligned = self.decoder(query, video_features)\n",
        "        text_aligned = self.decoder(query, text_features)\n",
        "\n",
        "        return video_aligned, text_aligned\n",
        "\n",
        "# 3. Re-initialize the model with updated components\n",
        "model = CRET().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQu9X23wnItO"
      },
      "outputs": [],
      "source": [
        "def retrieve_video(query_text, top_k=5):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Initialize tokenizer here\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Process query\n",
        "    tokens = tokenizer(\n",
        "        query_text,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_YuiG8NIA7z"
      },
      "source": [
        "## Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uObOl6rFINuk",
        "outputId": "4834c181-53a8-4527-b7e0-4990c4a21d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.9.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.5.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPX1psRoWwb_"
      },
      "outputs": [],
      "source": [
        "def retrieve_video_from_larger_set(query, num_videos=100, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieves the top-k most relevant video paths from a larger set of videos based on the given query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The text query to search for.\n",
        "        num_videos (int): The number of videos to consider in the larger set.\n",
        "        top_k (int): The number of most relevant videos to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, where each tuple contains the video path and the similarity score.\n",
        "    \"\"\"\n",
        "    # Implement the CRET model logic to retrieve the top-k most relevant video paths\n",
        "    # This might involve:\n",
        "    # 1. Encoding the query using a text encoder\n",
        "    # 2. Encoding the video metadata (titles, descriptions, etc.) using the same encoder\n",
        "    # 3. Calculating the similarity between the query and the video metadata\n",
        "    # 4. Sorting the videos by similarity and returning the top-k results\n",
        "\n",
        "    # For now, this is a placeholder implementation that simply returns a list of random video paths\n",
        "    import random\n",
        "    video_paths = [f\"video_{i}.mp4\" for i in range(num_videos)]\n",
        "    similarities = [random.uniform(0, 1) for _ in range(num_videos)]\n",
        "\n",
        "    # Sort the results by similarity in descending order\n",
        "    sorted_results = sorted(zip(video_paths, similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_results[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95pyyM9mXE7c"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def process_video_for_display(video_path, max_frames=6):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Could not open video: {video_path}\")\n",
        "            return []\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if total_frames == 0:\n",
        "            print(f\"Video {video_path} has no frames.\")\n",
        "            return []\n",
        "\n",
        "        indices = np.linspace(0, total_frames-1, max_frames, dtype=int)\n",
        "\n",
        "        frames = []\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(Image.fromarray(frame))\n",
        "            else:\n",
        "                print(f\"Error reading frame {idx} from video {video_path}\")\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_path}: {str(e)}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKgIyQzKXOGb"
      },
      "outputs": [],
      "source": [
        "def video_search(query, num_results=5):\n",
        "    try:\n",
        "        # Process query through CRET model\n",
        "        results = retrieve_video_from_larger_set(query, num_videos=100, top_k=num_results)\n",
        "\n",
        "        if not results:\n",
        "            return [], \"No results found\"\n",
        "\n",
        "        # Process results for display\n",
        "        output_frames = []\n",
        "        captions = []\n",
        "\n",
        "        for video_path, similarity in results:\n",
        "            frames = process_video_for_display(video_path)\n",
        "            if frames:\n",
        "                output_frames.extend(frames)\n",
        "                video_name = os.path.basename(video_path)\n",
        "                captions.extend([f\"{video_name} (similarity: {similarity:.3f})\"] * len(frames))\n",
        "\n",
        "        if not output_frames:\n",
        "            return [], \"No frames could be extracted\"\n",
        "\n",
        "        return output_frames, f\"Found {len(results)} matching videos\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return [], f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "AzTc0IL4adZM",
        "outputId": "d6d8a8bb-166b-4b3b-cfa1-1168bb2a309d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-894bb895-816f-4b65-8cc8-cf0cca23bf86\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-894bb895-816f-4b65-8cc8-cf0cca23bf86\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Follow the prompt to upload your video files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM-vP6jd6MeQ"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWHAMddv6XaA",
        "outputId": "67df0d93-e65b-4c67-8255-6f2a4c7137e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to open video file at: /content/videos/video1.mp4\n",
            "Could not open video: videos/video1.mp4\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Specify the path to your video file\n",
        "video_path = 'videos/video1.mp4'  # Adjust this path accordingly\n",
        "\n",
        "# Print absolute path for debugging\n",
        "print(f\"Attempting to open video file at: {os.path.abspath(video_path)}\")\n",
        "\n",
        "# Create a VideoCapture object\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if the video opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Could not open video: {video_path}\")\n",
        "else:\n",
        "    print(\"Video opened successfully.\")\n",
        "\n",
        "# Read and display the video frame by frame\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"End of video or cannot read the video frame.\")\n",
        "        break\n",
        "\n",
        "    # Display the frame in a window\n",
        "    cv2.imshow('Video', frame)\n",
        "\n",
        "    # Press 'q' on the keyboard to exit the video early\n",
        "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close all OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU7nZ6tQb88I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Sample video database\n",
        "video_database = [\n",
        "    {\"path\": \"MSRVTT/video1.mp4\", \"tags\": [\"cooking\", \"food\"]},\n",
        "    {\"path\": \"MSRVTT/video2.mp4\", \"tags\": [\"dancing\", \"party\", \"performance\"]},\n",
        "    {\"path\": \"MSRVTT/video3.mp4\", \"tags\": [\"sports\", \"exercise\", \"game\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_lHGzVJ8ZNW"
      },
      "outputs": [],
      "source": [
        "def process_video_for_display(video_path, max_frames=6):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open video '{video_path}'.\")\n",
        "            return []\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if total_frames == 0:\n",
        "            print(f\"Video '{video_path}' has no frames.\")\n",
        "            return []\n",
        "\n",
        "        # Ensure we have at least 1 frame\n",
        "        if total_frames < max_frames:\n",
        "            max_frames = total_frames\n",
        "\n",
        "        indices = np.linspace(0, total_frames-1, max_frames, dtype=int)\n",
        "\n",
        "        frames = []\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(Image.fromarray(frame))\n",
        "            else:\n",
        "                print(f\"Error reading frame {idx} from video '{video_path}'.\")\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video '{video_path}': {str(e)}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pFx9QfL8byt"
      },
      "outputs": [],
      "source": [
        "def retrieve_video_from_larger_set(query, num_videos=1, top_k=5):\n",
        "    relevant_videos = []\n",
        "    query_tags = query.lower().split()  # Tokenize the input query\n",
        "\n",
        "    print(f\"Query: {query} | Query Tags: {query_tags}\")  # Debugging purpose\n",
        "\n",
        "    for video in video_database:\n",
        "        # Check if any of the query tags are in the video's tags\n",
        "        if any(tag in video['tags'] for tag in query_tags):\n",
        "            relevant_videos.append((video['path'], 1.0))  # Mock similarity score\n",
        "            print(f\"Match found: {video}\")  # Debugging\n",
        "\n",
        "    # Sort by similarity score (if there were real scores) and take the top k results\n",
        "    relevant_videos.sort(key=lambda x: x[1], reverse=True)\n",
        "    return relevant_videos[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byMGRWin8eg1"
      },
      "outputs": [],
      "source": [
        "def video_search(query, num_results=5):\n",
        "    try:\n",
        "        results = retrieve_video_from_larger_set(query, top_k=num_results)\n",
        "\n",
        "        if not results:\n",
        "            return [], \"No results found.\"\n",
        "\n",
        "        output_frames = []\n",
        "        captions = []\n",
        "\n",
        "        for video_path, similarity in results:\n",
        "            frames = process_video_for_display(video_path)\n",
        "            if frames:\n",
        "                output_frames.extend(frames)\n",
        "                video_name = os.path.basename(video_path)\n",
        "                captions.extend([f\"{video_name} (similarity: {similarity:.3f})\"] * len(frames))\n",
        "            else:\n",
        "                print(f\"Warning: No frames extracted from video '{video_path}'.\")\n",
        "\n",
        "        if not output_frames:\n",
        "            return [], \"No frames could be extracted.\"\n",
        "\n",
        "        return output_frames, f\"Found {len(results)} matching videos.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return [], f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsPtXVah8_vb",
        "outputId": "af044469-4908-4ad9-d28d-c9480a8c3f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: cooking | Query Tags: ['cooking']\n",
            "Match found: {'path': 'videos/video1.mp4', 'tags': ['cooking', 'food']}\n",
            "Error: Could not open video 'videos/video1.mp4'.\n",
            "Warning: No frames extracted from video 'videos/video1.mp4'.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "output_frames, status = video_search(\"cooking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "T6hMut338hE9",
        "outputId": "4dab7ae0-700b-44da-c46f-ebd9d6ccdc4d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'gr' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aaabdca6443d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Gradio Interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Video Search\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     gr.Markdown(\"\"\"\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Video Search using CRET Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSearch\u001b[0m \u001b[0mthrough\u001b[0m \u001b[0mvideos\u001b[0m \u001b[0musing\u001b[0m \u001b[0mnatural\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
          ]
        }
      ],
      "source": [
        "# Gradio Interface\n",
        "with gr.Blocks(title=\"Video Search\", theme=gr.themes.Base()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Video Search using CRET Model\n",
        "    Search through videos using natural language descriptions.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            query_input = gr.Textbox(\n",
        "                label=\"Enter your text query\",\n",
        "                placeholder=\"Example: a person is cooking food\",\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "            example_queries = gr.Examples(\n",
        "                examples=[\n",
        "                    \"cooking\",\n",
        "                    \"dancing\",\n",
        "                    \"sports\"\n",
        "                ],\n",
        "                inputs=query_input,\n",
        "                label=\"Example Queries\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            num_results = gr.Slider(\n",
        "                minimum=1,\n",
        "                maximum=10,\n",
        "                value=5,\n",
        "                step=1,\n",
        "                label=\"Number of results\"\n",
        "            )\n",
        "\n",
        "    search_button = gr.Button(\"Search\", variant=\"primary\")\n",
        "\n",
        "    status = gr.Textbox(label=\"Status\", value=\"Ready\", interactive=False)\n",
        "\n",
        "    gallery = gr.Gallery(\n",
        "        label=\"Results\",\n",
        "        show_label=True,\n",
        "        columns=3,\n",
        "        rows=None,\n",
        "        height=\"500px\"\n",
        "    )\n",
        "\n",
        "    search_button.click(\n",
        "        fn=video_search,\n",
        "        inputs=[query_input, num_results],\n",
        "        outputs=[gallery, status]\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "078df586003a46e984efd249ebeca620": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de0ee00cfbf04a0fbf1940afbc593fbd",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e653316f2ae40348653a7a1bb564d3c",
            "value": 48
          }
        },
        "091fe204837441d2aed7fe2fbc7cc6e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b178540e0d54b74927a06fd930be320",
            "placeholder": "​",
            "style": "IPY_MODEL_47941bbb99b341e5b2189376ee914fba",
            "value": " 570/570 [00:00&lt;00:00, 18.1kB/s]"
          }
        },
        "0986586914834c6794ee308eacd775e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2175d2bc434e4edcb4990513a63e53ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "262cfdc0e3aa46ae8a1fda3514d89ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fda19017ec14e0d8faa29ff6a6eaf15",
            "placeholder": "​",
            "style": "IPY_MODEL_eabc566ddc194ba782508d4ed9f5cadd",
            "value": " 466k/466k [00:00&lt;00:00, 10.4MB/s]"
          }
        },
        "2d7d09eff1db4d4cbab3a233b20f019e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "366873965c5741b285403ad21fc76c87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36fc0dffeb7c4974a1d4c48fd7da763a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e0e1e90dfee42b597a2f6db21ef62a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4778cb76d4d740c0a274da5c71829986": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47941bbb99b341e5b2189376ee914fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ed5db89b2664304bd9bd77796e8c393": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b178540e0d54b74927a06fd930be320": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f9744a72db94c54b3cb146d0ba5b259": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c868fb2756334fdfa509d53367dd0314",
              "IPY_MODEL_7760cc4bc097434395671a68be9e322c",
              "IPY_MODEL_262cfdc0e3aa46ae8a1fda3514d89ede"
            ],
            "layout": "IPY_MODEL_9877a0c76354462bbe1f88107b354fc3"
          }
        },
        "6bbed8b1874a48bfa161a0fa59e5075d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d1dd8c890ca4f8fb644908def62dfd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e653316f2ae40348653a7a1bb564d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fda19017ec14e0d8faa29ff6a6eaf15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71557515b017488182ccac996f3e3c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deee248424164371a0019036677d69db",
            "placeholder": "​",
            "style": "IPY_MODEL_d8284d01b1804ba1a4a1733e7f23a7fe",
            "value": " 232k/232k [00:00&lt;00:00, 3.58MB/s]"
          }
        },
        "7760cc4bc097434395671a68be9e322c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f33236c245fa4e0488724681965de166",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_985381c3cc214d299ec2a390c4dd5857",
            "value": 466062
          }
        },
        "7884f759f8b849a39079b9048e2d8c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d1dd8c890ca4f8fb644908def62dfd9",
            "placeholder": "​",
            "style": "IPY_MODEL_a6c594d3820c43e98341b1842364c434",
            "value": "model.safetensors: 100%"
          }
        },
        "8381bf6400f54d428aa9871b9379f2ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880f12566a634dd29ada4e5cf7f30a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c194d4304cb453eaf4f040adca5760a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d148c6f3c096434da0c72cfb4a2f02a8",
              "IPY_MODEL_dd411e6422cb48d794f06d505e0a68d1",
              "IPY_MODEL_71557515b017488182ccac996f3e3c43"
            ],
            "layout": "IPY_MODEL_8381bf6400f54d428aa9871b9379f2ab"
          }
        },
        "8d895abbaee04237b6c6bf684e9f60db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36fc0dffeb7c4974a1d4c48fd7da763a",
            "placeholder": "​",
            "style": "IPY_MODEL_880f12566a634dd29ada4e5cf7f30a8c",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.10kB/s]"
          }
        },
        "9137c6d0134e4a4db7b8c9b63fcbd2ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de0a747d43bb46d9a35258e83d29ef53",
              "IPY_MODEL_078df586003a46e984efd249ebeca620",
              "IPY_MODEL_8d895abbaee04237b6c6bf684e9f60db"
            ],
            "layout": "IPY_MODEL_4ed5db89b2664304bd9bd77796e8c393"
          }
        },
        "91f3051b7dc44da78da8ab516f7bbff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97d1577a08ef4fa7aac31dc7d1270eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d7d09eff1db4d4cbab3a233b20f019e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_beb42a934644434c9c62633b469b8bd0",
            "value": 570
          }
        },
        "985381c3cc214d299ec2a390c4dd5857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9877a0c76354462bbe1f88107b354fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f7b35229bb44583ba3b683e98eb13c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a02604c388f24c1fbfc04b4277ca3ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bbed8b1874a48bfa161a0fa59e5075d",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8ac2448be6f4f598f74018c5188fec2",
            "value": 440449768
          }
        },
        "a6c594d3820c43e98341b1842364c434": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ada9f719fd2e4130816dc65625c5d37c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0d60b63b05e488baffff237f140abd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ee5338e874420eba594476c274159c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f16450b38ec449059eef3942da4a498c",
              "IPY_MODEL_97d1577a08ef4fa7aac31dc7d1270eb1",
              "IPY_MODEL_091fe204837441d2aed7fe2fbc7cc6e8"
            ],
            "layout": "IPY_MODEL_fc56f935fa6c4fe9b70a50f22bd24694"
          }
        },
        "bd0dc8efd78c42a2b11516325eb3294c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb42a934644434c9c62633b469b8bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c868fb2756334fdfa509d53367dd0314": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4778cb76d4d740c0a274da5c71829986",
            "placeholder": "​",
            "style": "IPY_MODEL_bd0dc8efd78c42a2b11516325eb3294c",
            "value": "tokenizer.json: 100%"
          }
        },
        "d03a96d433f34598949e8e253a3332f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d148c6f3c096434da0c72cfb4a2f02a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df6c0a292fa44ddc82c3ece0aace2e92",
            "placeholder": "​",
            "style": "IPY_MODEL_2175d2bc434e4edcb4990513a63e53ca",
            "value": "vocab.txt: 100%"
          }
        },
        "d8284d01b1804ba1a4a1733e7f23a7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd411e6422cb48d794f06d505e0a68d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f7b35229bb44583ba3b683e98eb13c6",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9d91c05721647ab9aa60cd569509f02",
            "value": 231508
          }
        },
        "de0a747d43bb46d9a35258e83d29ef53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ada9f719fd2e4130816dc65625c5d37c",
            "placeholder": "​",
            "style": "IPY_MODEL_3e0e1e90dfee42b597a2f6db21ef62a8",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "de0ee00cfbf04a0fbf1940afbc593fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deee248424164371a0019036677d69db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6c0a292fa44ddc82c3ece0aace2e92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea1e5e2d27f34ce6b59b9aceed530a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d03a96d433f34598949e8e253a3332f0",
            "placeholder": "​",
            "style": "IPY_MODEL_0986586914834c6794ee308eacd775e0",
            "value": " 440M/440M [00:07&lt;00:00, 15.6MB/s]"
          }
        },
        "eabc566ddc194ba782508d4ed9f5cadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0da983287c3494fac27f8815cc5b8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7884f759f8b849a39079b9048e2d8c34",
              "IPY_MODEL_a02604c388f24c1fbfc04b4277ca3ce1",
              "IPY_MODEL_ea1e5e2d27f34ce6b59b9aceed530a99"
            ],
            "layout": "IPY_MODEL_366873965c5741b285403ad21fc76c87"
          }
        },
        "f16450b38ec449059eef3942da4a498c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0d60b63b05e488baffff237f140abd4",
            "placeholder": "​",
            "style": "IPY_MODEL_91f3051b7dc44da78da8ab516f7bbff9",
            "value": "config.json: 100%"
          }
        },
        "f33236c245fa4e0488724681965de166": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8ac2448be6f4f598f74018c5188fec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9d91c05721647ab9aa60cd569509f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc56f935fa6c4fe9b70a50f22bd24694": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}